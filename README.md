# Skip-gram model

The Skip-gram model, along with continuous-bag of words(CBOW), is used frequently in NLP using deep learning.
Given a center(current) word, it tries to predict range N of context words before and after the target word.

This code is an implementation of a blog written by Mateusz Bednarski in Medium: Implementing word2vec in PyTorch.

	https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb


## References

For further studies about word embeddings, read the papers below:

1. Efficient Estimation of Word Representations in Vector Space
2. word2vec Explained: Deriving Mikolov et al's Negative-Sampling Word-Embedding Method
3. Distributed Representations of Words and Phrases and their Compositionality



 
